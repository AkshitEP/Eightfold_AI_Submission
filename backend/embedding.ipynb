{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritulkumawat/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSON file\n",
    "def load_json(i):\n",
    "    with open(f\"./str_data_resumes/{i}.json\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract work descriptions from the resume JSON data\n",
    "def extract_work_descriptions(data):\n",
    "    work_descriptions = []\n",
    "    # Check if job_history exists in the data\n",
    "    if \"details\" in data and \"job_history\" in data[\"details\"]:\n",
    "        for job in data[\"details\"][\"job_history\"]:\n",
    "            if \"work_description\" in job:\n",
    "                work_descriptions.append(job[\"work_description\"])\n",
    "    return work_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "# Function to get embeddings for a list of work descriptions\n",
    "def get_embedding(text):\n",
    "    # Generate embedding for the given text\n",
    "    embedding = model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "# Store all embeddings in a dictionary\n",
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to store embeddings incrementally\n",
    "embeddings_file = \"embeddings.jsonl\"\n",
    "\n",
    "# Load existing embeddings if the file already exists (resume functionality)\n",
    "processed_files = set()\n",
    "if os.path.exists(embeddings_file):\n",
    "    with open(embeddings_file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            # Read and parse each line as JSON\n",
    "            record = json.loads(line)\n",
    "            processed_files.add(record['file_index'])\n",
    "\n",
    "# Start from the last processed file\n",
    "start_index = len(processed_files)\n",
    "\n",
    "# Loop through files 0.json to 999.json and show progress bar\n",
    "with open(embeddings_file, \"a\") as outfile:  # Open in append mode\n",
    "    for i in tqdm(range(start_index, 1000), initial=start_index, total=1000, desc=\"Processing Resumes\"):\n",
    "        try:\n",
    "            if i in processed_files:\n",
    "                continue  # Skip already processed files\n",
    "            \n",
    "            # Load the resume data\n",
    "            resume = load_json(i)\n",
    "            # Extract work descriptions\n",
    "            work_descriptions_list = extract_work_descriptions(resume)\n",
    "            \n",
    "            # Generate embeddings for each work description\n",
    "            for desc_index, desc in enumerate(work_descriptions_list):\n",
    "                embedding = get_embedding(desc)\n",
    "                \n",
    "                # Prepare the JSON object to write\n",
    "                record = {\n",
    "                    \"file_index\": i,\n",
    "                    \"desc_index\": desc_index,\n",
    "                    \"work_description\": desc,\n",
    "                    \"embedding\": embedding.tolist()  # Convert embedding to list for JSON\n",
    "                }\n",
    "                \n",
    "                # Write the JSON object to the JSONL file\n",
    "                outfile.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {i}.json: {e}\")\n",
    "\n",
    "print(f\"Embeddings saved to {embeddings_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "# Function to load all embeddings from the JSONL file\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = []\n",
    "    with open(file_path, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            embeddings.append({\n",
    "                \"file_index\": data[\"file_index\"],\n",
    "                \"desc_index\": data[\"desc_index\"],\n",
    "                \"work_description\": data[\"work_description\"],\n",
    "                \"embedding\": np.array(data[\"embedding\"])  # Convert list back to NumPy array\n",
    "            })\n",
    "    return embeddings\n",
    "\n",
    "# Load all embeddings\n",
    "embeddings_file = \"embeddings.jsonl\"\n",
    "embeddings_data = load_embeddings(embeddings_file)\n",
    "\n",
    "# Function to compute cosine similarity between query and all embeddings\n",
    "def query_system(user_query, top_n=100):\n",
    "    # Convert user query to embedding\n",
    "    query_embedding = model.encode(user_query)\n",
    "    \n",
    "    # List to store cosine similarities\n",
    "    similarities = []\n",
    "    \n",
    "    # Compute cosine similarity with each embedding\n",
    "    for i, entry in enumerate(embeddings_data):\n",
    "        similarity = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1), \n",
    "            entry[\"embedding\"].reshape(1, -1)\n",
    "        )[0][0]  # Flatten similarity value\n",
    "        similarities.append((similarity, entry))\n",
    "    \n",
    "    # Find top N results with the highest cosine similarity\n",
    "    top_results = heapq.nlargest(top_n, similarities, key=lambda x: x[0])\n",
    "    \n",
    "    # Count occurrences of each file_index in top results\n",
    "    file_index_counter = Counter([entry[\"file_index\"] for _, entry in top_results])\n",
    "    \n",
    "    # Sort file indices by frequency\n",
    "    sorted_file_indices = file_index_counter.most_common()\n",
    "    \n",
    "    # Display top file indices and corresponding work descriptions\n",
    "    print(\"\\nTop matching file indices:\")\n",
    "    for file_index, count in sorted_file_indices:\n",
    "        print(f\"\\nFile index {file_index} appeared {count} times.\")\n",
    "        # Display all work descriptions related to this file_index in the top results\n",
    "        for _, entry in top_results:\n",
    "            if entry[\"file_index\"] == file_index:\n",
    "                print(f\"- Description: {entry['work_description']}\")\n",
    "    \n",
    "    return sorted_file_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "user_query = \"people having software development experience\"\n",
    "top_results = query_system(user_query)\n",
    "\n",
    "top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load all embeddings from the JSONL file\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = []\n",
    "    with open(file_path, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            embeddings.append({\n",
    "                \"file_index\": data[\"file_index\"],\n",
    "                \"desc_index\": data[\"desc_index\"],\n",
    "                \"work_description\": data[\"work_description\"],\n",
    "                \"embedding\": np.array(data[\"embedding\"])  # Convert list back to NumPy array\n",
    "            })\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BM25 corpus (text descriptions)\n",
    "def load_bm25_corpus(embeddings_data):\n",
    "    corpus = []\n",
    "    for entry in embeddings_data:\n",
    "        tokens = nltk.word_tokenize(entry['work_description'].lower())\n",
    "        corpus.append(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 search function\n",
    "def bm25_search(bm25, query, corpus, top_n=100):\n",
    "    query_tokens = nltk.word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    top_indices = np.argsort(scores)[-top_n:][::-1]  # Get top N indices (descending)\n",
    "    return [(corpus[i], scores[i], i) for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity search function\n",
    "def cosine_similarity_search(user_query_embedding, embeddings_data, top_n=100):\n",
    "    similarities = []\n",
    "    \n",
    "    for i, entry in enumerate(embeddings_data):\n",
    "        similarity = cosine_similarity(\n",
    "            user_query_embedding.reshape(1, -1), \n",
    "            entry[\"embedding\"].reshape(1, -1)\n",
    "        )[0][0]  # Flatten similarity value\n",
    "        similarities.append((similarity, entry))\n",
    "    \n",
    "    # Find top N results with the highest cosine similarity\n",
    "    top_results = heapq.nlargest(top_n, similarities, key=lambda x: x[0])\n",
    "    \n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize scores between 0 and 1\n",
    "def normalize(scores):\n",
    "    min_val = min(scores)\n",
    "    max_val = max(scores)\n",
    "    return [(score - min_val) / (max_val - min_val) for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(user_query, embeddings_data, corpus, bm25, top_n=100, bm25_weight=0.4, cosine_weight=0.6):\n",
    "    # Generate embedding for user query\n",
    "    query_embedding = model.encode(user_query)\n",
    "    \n",
    "    # Perform BM25 search\n",
    "    bm25_results = bm25_search(bm25, user_query, corpus, top_n)\n",
    "    bm25_scores = [score for _, score, _ in bm25_results]\n",
    "    normalized_bm25_scores = normalize(bm25_scores)\n",
    "    \n",
    "    # Perform cosine similarity search\n",
    "    cosine_results = cosine_similarity_search(query_embedding, embeddings_data, top_n)\n",
    "    cosine_scores = [similarity for similarity, _ in cosine_results]\n",
    "    normalized_cosine_scores = normalize(cosine_scores)\n",
    "    \n",
    "    # Combine BM25 and Cosine Similarity scores\n",
    "    combined_results = []\n",
    "    for i in range(top_n):\n",
    "        bm25_score = normalized_bm25_scores[i]\n",
    "        cosine_score = normalized_cosine_scores[i]\n",
    "        combined_score = bm25_weight * bm25_score + cosine_weight * cosine_score\n",
    "        \n",
    "        # Find the corresponding entry in embeddings_data using the index from BM25 results\n",
    "        entry = embeddings_data[bm25_results[i][2]]  # Use index from BM25 results\n",
    "        combined_results.append((combined_score, cosine_scores[i], entry))  # Store combined score and original cosine score\n",
    "    \n",
    "    # Sort combined results by cosine similarity score (second item in tuple) in descending order\n",
    "    combined_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display top results based on maximum cosine similarity score\n",
    "    print(\"\\nTop matching results based on Cosine Similarity:\")\n",
    "    for score, cosine_sim, entry in combined_results:\n",
    "        print(f\"\\nCosine Similarity: {cosine_sim:.4f}, Combined Score: {score:.4f}\")\n",
    "        print(f\"File Index: {entry['file_index']}\")\n",
    "        print(f\"- Description: {entry['work_description']}\")\n",
    "    \n",
    "    return combined_results  # Return combined results for further processing if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings data and prepare BM25 corpus\n",
    "embeddings_file = \"embeddings.jsonl\"\n",
    "embeddings_data = load_embeddings(embeddings_file)\n",
    "corpus = load_bm25_corpus(embeddings_data)\n",
    "    \n",
    "# Initialize BM25 with the corpus\n",
    "bm25 = BM25Okapi(corpus)\n",
    "    \n",
    "# User query\n",
    "user_query = \"Find me a professor\"\n",
    "    \n",
    "# Perform hybrid search\n",
    "hybrid_results = hybrid_search(user_query, embeddings_data, corpus, bm25, top_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
